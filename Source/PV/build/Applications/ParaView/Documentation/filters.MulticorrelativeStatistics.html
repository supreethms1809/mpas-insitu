
<html>
    <head>
        <title>Multicorrelative Statistics</title>
        <meta name="proxy_name" contents="filters.MulticorrelativeStatistics"/>
        <meta name="filename" contents="filters.MulticorrelativeStatistics.html"/>
    </head>
    <body>
        <h2>Multicorrelative Statistics (MulticorrelativeStatistics)</h2>
        <i>
            <p>Compute a statistical model of a dataset and/or assess the dataset with a statistical model.</p>
        </i>
        <div class="description">
      This filter either computes a statistical model of a dataset or takes
      such a model as its second input. Then, the model (however it is
      obtained) may optionally be used to assess the input dataset.&lt;p&gt;
      This filter computes the covariance matrix for all the arrays you select
      plus the mean of each array. The model is thus a multivariate Gaussian
      distribution with the mean vector and variances provided. Data is
      assessed using this model by computing the Mahalanobis distance for each
      input point. This distance will always be positive.&lt;p&gt; The learned
      model output format is rather dense and can be confusing, so it is
      discussed here. The first filter output is a multiblock dataset
      consisting of 2 tables: &lt;ol&gt; &lt;li&gt; Raw covariance data.
      &lt;li&gt; Covariance matrix and its Cholesky decomposition. &lt;/ol&gt;
      The raw covariance table has 3 meaningful columns: 2 titled "Column1" and
      "Column2" whose entries generally refer to the N arrays you selected when
      preparing the filter and 1 column titled "Entries" that contains numeric
      values. The first row will always contain the number of observations in
      the statistical analysis. The next N rows contain the mean for each of
      the N arrays you selected. The remaining rows contain covariances of
      pairs of arrays.&lt;p&gt; The second table (covariance matrix and
      Cholesky decomposition) contains information derived from the raw
      covariance data of the first table. The first N rows of the first column
      contain the name of one array you selected for analysis. These rows are
      followed by a single entry labeled "Cholesky" for a total of N+1 rows.
      The second column, Mean contains the mean of each variable in the first N
      entries and the number of observations processed in the final (N+1)
      row.&lt;p&gt; The remaining columns (there are N, one for each array)
      contain 2 matrices in triangular format. The upper right triangle
      contains the covariance matrix (which is symmetric, so its lower triangle
      may be inferred). The lower left triangle contains the Cholesky
      decomposition of the covariance matrix (which is triangular, so its upper
      triangle is zero). Because the diagonal must be stored for both matrices,
      an additional row is required â€” hence the N+1 rows and
      the final entry of the column named "Column".</div>
        <table width="97%" border="2px">
            <tr bgcolor="#9acd32">
                <th>Property</th>
                <th width="60%">Description</th>
                <th width="5%">Default(s)</th>
                <th width="20%">Restrictions</th>
            </tr>
            <tr>
                <th>Input</th>
                <td>The input to the filter. Arrays from this dataset will
        be used for computing statistics and/or assessed by a statistical
        model.</td>
                <td>
                    <span/>
                </td>
                <td>
                    <p>Accepts input of following types:<ul>
                            <li>vtkImageData</li>
                            <li>vtkStructuredGrid</li>
                            <li>vtkPolyData</li>
                            <li>vtkUnstructuredGrid</li>
                            <li>vtkTable</li>
                            <li>vtkGraph</li>
                        </ul>
                    </p>
                    <p>
      The dataset much contain a field array ()
      </p>
                </td>
            </tr>
            <tr>
                <th>ModelInput</th>
                <td>A previously-calculated model with which to assess a
        separate dataset. This input is optional.</td>
                <td>
                    <span/>
                </td>
                <td>
                    <p>Accepts input of following types:<ul>
                            <li>vtkTable</li>
                            <li>vtkMultiBlockDataSet</li>
                        </ul>
                    </p>
                </td>
            </tr>
            <tr>
                <th>AttributeMode</th>
                <td>Specify which type of field data the arrays will be
        drawn from.</td>
                <td>0<span/>
                </td>
                <td>
                    <p>The value must be field array name.</p>
                </td>
            </tr>
            <tr>
                <th>Variables of Interest</th>
                <td>Choose arrays whose entries will be used to form
        observations for statistical analysis.</td>
                <td>
                    <span/>
                </td>
                <td/>
            </tr>
            <tr>
                <th>Task</th>
                <td>Specify the task to be performed: modeling and/or
        assessment. &lt;ol&gt; &lt;li&gt; "Detailed model of input data,"
        creates a set of output tables containing a calculated statistical
        model of the &lt;b&gt;entire&lt;/b&gt; input dataset;&lt;/li&gt;
        &lt;li&gt; "Model a subset of the data," creates an output table (or
        tables) summarizing a &lt;b&gt;randomly-chosen subset&lt;/b&gt; of the
        input dataset;&lt;/li&gt; &lt;li&gt; "Assess the data with a model,"
        adds attributes to the first input dataset using a model provided on
        the second input port; and&lt;/li&gt; &lt;li&gt; "Model and assess the
        same data," is really just operations 2 and 3 above applied to the same
        input dataset. The model is first trained using a fraction of the input
        data and then the entire dataset is assessed using that
        model.&lt;/li&gt; &lt;/ol&gt; When the task includes creating a model
        (i.e., tasks 2, and 4), you may adjust the fraction of the input
        dataset used for training. You should avoid using a large fraction of
        the input data for training as you will then not be able to detect
        overfitting. The &lt;i&gt;Training fraction&lt;/i&gt; setting will be
        ignored for tasks 1 and 3.</td>
                <td>3<span/>
                </td>
                <td>
                    <p>The value(s) is an enumeration of the following:<ul>
                            <li>Detailed model of input data (0)</li>
                            <li>Model a subset of the data (1)</li>
                            <li>Assess the data with a model (2)</li>
                            <li>Model and assess the same data (3)</li>
                        </ul>
                    </p>
                </td>
            </tr>
            <tr>
                <th>TrainingFraction</th>
                <td>Specify the fraction of values from the input dataset to
        be used for model fitting. The exact set of values is chosen at random
        from the dataset.</td>
                <td>0.1<span/>
                </td>
                <td/>
            </tr>
        </table>
    </body>
</html>